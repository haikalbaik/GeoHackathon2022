{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('_train.csv', sep=\";\")\n",
    "df.fillna(-999.25, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['WELL', 'DEPTH_MD', 'X_LOC', 'Y_LOC', 'Z_LOC', 'GROUP', 'FORMATION',\n",
      "       'CALI', 'RSHA', 'RMED', 'RDEP', 'RHOB', 'GR', 'SGR', 'NPHI', 'PEF',\n",
      "       'DTC', 'SP', 'BS', 'ROP', 'DTS', 'DCAL', 'DRHO', 'MUDWEIGHT', 'RMIC',\n",
      "       'ROPA', 'RXO', 'FACIES_LITHOLOGY', 'FACIES_CONFIDENCE'],\n",
      "      dtype='object')\n",
      "\n",
      "Number of Lithology: 12\n",
      "\n",
      "Number of Wells: 98\n",
      "\n",
      "Number of Features: 29\n",
      "Nullvalue in WELL : 0\n",
      "Nullvalue in DEPTH_MD : 0\n",
      "Nullvalue in X_LOC : 0\n",
      "Nullvalue in Y_LOC : 0\n",
      "Nullvalue in Z_LOC : 0\n",
      "Nullvalue in GROUP : 0\n",
      "Nullvalue in FORMATION : 0\n",
      "Nullvalue in CALI : 0\n",
      "Nullvalue in RSHA : 0\n",
      "Nullvalue in RMED : 0\n",
      "Nullvalue in RDEP : 0\n",
      "Nullvalue in RHOB : 0\n",
      "Nullvalue in GR : 0\n",
      "Nullvalue in SGR : 0\n",
      "Nullvalue in NPHI : 0\n",
      "Nullvalue in PEF : 0\n",
      "Nullvalue in DTC : 0\n",
      "Nullvalue in SP : 0\n",
      "Nullvalue in BS : 0\n",
      "Nullvalue in ROP : 0\n",
      "Nullvalue in DTS : 0\n",
      "Nullvalue in DCAL : 0\n",
      "Nullvalue in DRHO : 0\n",
      "Nullvalue in MUDWEIGHT : 0\n",
      "Nullvalue in RMIC : 0\n",
      "Nullvalue in ROPA : 0\n",
      "Nullvalue in RXO : 0\n",
      "Nullvalue in FACIES_LITHOLOGY : 0\n",
      "Nullvalue in FACIES_CONFIDENCE : 0\n"
     ]
    }
   ],
   "source": [
    "#Renaming the lithology and confidence to something more understandable\n",
    "train = df.rename(columns={'FORCE_2020_LITHOFACIES_LITHOLOGY':'FACIES_LITHOLOGY','FORCE_2020_LITHOFACIES_CONFIDENCE':'FACIES_CONFIDENCE'})\n",
    "\n",
    "#Checking if column name successfully changed\n",
    "print(train.columns)\n",
    "\n",
    "#Checking how many unique target value \n",
    "print(\"\\nNumber of Lithology: \" + str(len(train.FACIES_LITHOLOGY.unique())))\n",
    "\n",
    "print(\"\\nNumber of Wells: \"+ str(len(train.WELL.unique())))\n",
    "\n",
    "print(\"\\nNumber of Features: {}\".format(len(train.columns)))\n",
    "\n",
    "for column in train.columns:\n",
    "  nullvalue=train[column].isna().sum()\n",
    "  print(\"Nullvalue in {} : {}\".format(column, nullvalue),)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cegal-welltools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The plot from cegaltools to show data coverage is definitely useful in terms of thinking how to deal with missing log values. \n",
    "# We need to investigate and deal with missing values before proceeding with Facies prediction\n",
    "from cegal.welltools.plotting import CegalWellPlotter as cwp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwp.plot_coverage(df=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating well plot given input. This was one using cegaltools library\n",
    "def show_well_curve(well_name):\n",
    "\n",
    "  #well = int(position)\n",
    "    well_name=well_name\n",
    "    df = train.loc[train.WELL == well_name].set_index('DEPTH_MD')\n",
    "    cwp.plot_logs(df=df, \n",
    "              logs=['GROUP','FORMATION', 'RHOB', 'GR', 'NPHI', 'DTC', 'DTS'], \n",
    "              log_scale_logs=['RMED', 'RDEP'],\n",
    "              lithology_logs='FACIES_LITHOLOGY', \n",
    "              lithology_proba_logs='FACIES_CONFIDENCE')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "widgets.interact(show_well_curve, well_name=train.WELL.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lithology_numbers = {30000: 0,\n",
    "                 65030: 1,\n",
    "                 65000: 2,\n",
    "                 80000: 3,\n",
    "                 74000: 4,\n",
    "                 70000: 5,\n",
    "                 70032: 6,\n",
    "                 88000: 7,\n",
    "                 86000: 8,\n",
    "                 99000: 9,\n",
    "                 90000: 10,\n",
    "                 93000: 11}\n",
    "\n",
    "train['FACIES_LITHOLOGY']=train['FACIES_LITHOLOGY'].map(lithology_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "formation = {-999.25:0, 'Utsira Fm.':1, 'Balder Fm.':2, 'Sele Fm.':3, 'Lista Fm.':4, 'Heimdal Fm.':5,\n",
    " 'Tor Fm.':6, 'Hod Fm.':7, 'Blodoeks Fm.':8, 'Svarte Fm.':9, 'Roedby Fm.':10, 'Sola Fm.':11,\n",
    " 'Aasgard Fm.':12, 'Draupne Fm.':13, 'Heather Fm.':14, 'Hugin Fm.':15, 'Smith Bank Fm.':16,\n",
    " 'Frigg Fm.':17, 'Skagerrak Fm.':18, 'Ekofisk Fm.':19, 'Kupferschiefer Fm.':20,\n",
    " 'Skade Fm.':21, 'Grid Fm.':22, 'Vaale Fm.':23, 'Sleipner Fm.':24, 'Hidra Fm.':25, 'Tuxen Fm.':26,\n",
    " 'Mandal Fm.':27, 'Ula Fm.':28, 'Bryne Fm.':29, 'Tau Fm.':30, 'Sandnes Fm.':31,\n",
    " 'Intra Draupne Fm. Sst.':32, 'Statfjord Fm.':33, 'Skade Mb.':34, 'BASEMENT':35,\n",
    " 'Ran Sst Mb.':36, 'Flekkefjord Fm.':37, 'Sauda Fm.':38, 'Egersund Fm.':39,\n",
    " 'Intra Balder Fm. Sst.':40, 'Hermod Mb.':41, 'Ty Fm.':42, 'Hardraade Fm.':43, 'Kyrre Fm.':44,\n",
    " 'Tryggvason Fm.':45, 'Drake Fm.':46, 'Cook Fm.':47, 'Amundsen Fm.':48, 'Grid Mb.':49,\n",
    " 'Ty Mb.':50, 'Jorsalfare Fm.':51, 'Burton Fm.':52, 'Mime Fm.':53,\n",
    " 'Intra Heather Fm. Sst.':54, 'Tarbert Fm.':55, 'Ness Fm.':56, 'Etive Fm.':57,\n",
    " 'Rannoch Fm.':58, 'Broom Fm.':59, 'Lunde Fm.':60, 'Oseberg Fm.':61, 'Sognefjord Fm.':62,\n",
    " 'Fensfjord Fm.':63, 'Krossfjord Fm.':64, 'Johansen Fm.':65, 'Eiriksson Mb.':66,\n",
    " 'Raude Mb.':67, 'Agat Fm.':68, 'Farsund Fm.':69}\n",
    "\n",
    "train['FORMATION']=train['FORMATION'].map(formation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Group = {'NORDLAND GP.':0, 'HORDALAND GP.':1, 'ROGALAND GP.':2, 'SHETLAND GP.':3,\n",
    "       'CROMER KNOLL GP.':4, 'VIKING GP.':5, 'VESTLAND GP.':6, 'ZECHSTEIN GP.':7,\n",
    "       'HEGRE GP.':8, 'ROTLIEGENDES GP.':9, 'TYNE GP.':10, 'BOKNFJORD GP.':11,\n",
    "       'DUNLIN GP.':12, 'BAAT GP.':13, -999.25:14}\n",
    "\n",
    "train['GROUP']=train['GROUP'].map(Group)\n",
    "\n",
    "train=train[train.GROUP != 14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.GROUP.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Load data\n",
    "#df_train = train\n",
    "\n",
    "# Data preprocessing\n",
    "X = train.drop(['DEPTH_MD', \n",
    "                           'X_LOC', \n",
    "                           'Y_LOC',\n",
    "                           'Z_LOC',\n",
    "                           'GROUP',\n",
    "                           'FORMATION',\n",
    "                           'SGR',\n",
    "                           'BS',\n",
    "                           'ROP',\n",
    "                           'DTS',\n",
    "                           'DCAL',\n",
    "                           'MUDWEIGHT','RMIC','ROPA','RXO'],axis=1)\n",
    "y = train['GROUP']\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to torch tensors\n",
    "# X_train = torch.FloatTensor(X_train)\n",
    "# X_test = torch.FloatTensor(X_test)\n",
    "# y_train = torch.LongTensor(y_train.values)\n",
    "# y_test = torch.LongTensor(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Model...........\n",
      "Predicting training dataset..............\n",
      "KFold Cross Validation in Progress........\n",
      "0.6675817949342946\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.47      0.62     22071\n",
      "           1       0.67      0.97      0.79     58596\n",
      "           2       0.76      0.33      0.46     26567\n",
      "           3       0.63      0.95      0.76     46861\n",
      "           4       0.94      0.09      0.16     10490\n",
      "           5       0.69      0.55      0.61     26418\n",
      "           6       1.00      0.08      0.16      5270\n",
      "           7       1.00      0.77      0.87      2412\n",
      "           8       0.00      0.00      0.00      2777\n",
      "           9       0.00      0.00      0.00       559\n",
      "          10       0.00      0.00      0.00       239\n",
      "          11       0.00      0.00      0.00       591\n",
      "          12       0.56      0.77      0.65     23822\n",
      "          13       1.00      0.00      0.00      7174\n",
      "\n",
      "    accuracy                           0.67    233847\n",
      "   macro avg       0.58      0.36      0.36    233847\n",
      "weighted avg       0.71      0.67      0.62    233847\n",
      "\n",
      "Accuracy after 10 folds is: 0.6668362952147446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "UsageError: Line magic function `%%timeit` not found.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "print('Preparing Model...........')\n",
    "rfcl = RandomForestClassifier(n_estimators = 300, random_state=1,max_features=12, max_depth=7, min_samples_leaf = 15)\n",
    "rfcl = rfcl.fit(X_train, y_train)\n",
    "\n",
    "from sklearn import metrics\n",
    "print('Predicting training dataset..............')\n",
    "y_predict = rfcl.predict(X_test)\n",
    "\n",
    "\n",
    "num_folds = 10\n",
    "seed = 77\n",
    "\n",
    "\n",
    "print('KFold Cross Validation in Progress........')\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "\n",
    "\n",
    "results = cross_val_score(rfcl,X, y, cv=kfold)\n",
    "\n",
    "average_accuracy = np.mean(abs(results))\n",
    "\n",
    "print(rfcl.score(X_test, y_test))\n",
    "print(classification_report(y_test, y_predict))\n",
    "\n",
    "print('Accuracy after {} folds is: {}'.format(num_folds, average_accuracy))\n",
    "\n",
    "%%timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['random_forestModelGROUP_1.joblib']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import joblib\n",
    "joblib.dump(rfcl, \"random_forestModelGROUP_1.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (pd.DataFrame(dTree.feature_importances_, columns = [\"Imp\"], index = X_train.columns))\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to torch tensors\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.LongTensor(y_train.values)\n",
    "y_test = torch.LongTensor(y_test.values)\n",
    "\n",
    "# Create dataset\n",
    "class ChurnDataset(Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "# Create dataloader\n",
    "train_data = ChurnDataset(X_train, y_train)\n",
    "test_data = ChurnDataset(X_test, y_test)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=10, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "sequence_length = 28\n",
    "input_size = 23\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 10\n",
    "batch_size = 1000\n",
    "num_epochs = 2\n",
    "learning_rate = 0.01\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) \n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dTree = DecisionTreeClassifier(criterion = 'entropy', max_depth=12)\n",
    "dTree.fit(X_train, y_train)\n",
    "dTree.fit(X_test, y_test)\n",
    "\n",
    "pred_train = dTree.predict(X_train)\n",
    "pred_test = dTree.predict(X_test)\n",
    "print(dTree.score(X_train, y_train))\n",
    "print(dTree.score(X_test, y_test))\n",
    "#print(pd.DataFrame(dTree.feature_importances_, columns = [\"Imp\"], \n",
    "#                   index = X_train.columns))#Print the feature importance of the decision model.Putting this in comment form \n",
    "\n",
    "print(classification_report(y_train, pred_train))\n",
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor, BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso \n",
    "\n",
    "models =[ KNeighborsRegressor(),\n",
    "          AdaBoostRegressor(),\n",
    "          GradientBoostingRegressor(),\n",
    "          RandomForestRegressor(),\n",
    "          LinearRegression(),\n",
    "          Lasso(),\n",
    "          DecisionTreeRegressor(),\n",
    "          Ridge()]\n",
    "        \n",
    "table = pd.DataFrame(columns = ['Model Name', 'Accuracy Score'])\n",
    "for classifier in models:\n",
    "    pipe = Pipeline(steps =[('models',classifier)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    pred = pipe.predict(X_test)\n",
    "    score = pipe.score(X_test, y_test)\n",
    "    result = ({'Model Name':classifier,\n",
    "               'Accuracy Score': score})\n",
    "    table = table.append(result, ignore_index=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knr = KNeighborsRegressor()\n",
    "knr_param = {\"n_neighbors\":[3,5,7],\n",
    "             \"weights\":['uniform','distance'],\n",
    "             \"algorithm\":['auto','kd_tree','brute'],\n",
    "            }\n",
    "\n",
    "gdb = GradientBoostingRegressor()\n",
    "gdb_param = {'learning_rate':[0.1,0.5,1],\n",
    "             'n_estimators':[100,200,400],\n",
    "             'min_samples_leaf': [1, 2, 4],\n",
    "             'min_samples_split': [2, 5, 10],\n",
    "             'max_features': ['auto','sqrt']}\n",
    "\n",
    "rfr = RandomForestRegressor()\n",
    "rfr_param = {'bootstrap': [True, False],\n",
    "             'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "             'max_features': ['auto', 'sqrt'],\n",
    "             'min_samples_leaf': [1, 2, 4],\n",
    "             'min_samples_split': [2, 5, 10],\n",
    "             'n_estimators': [200, 400, 600]}\n",
    "\n",
    "ls  = Lasso()\n",
    "ls_param = {'alpha':[0.1,0.2,0.5,0.8,1.0]}\n",
    "\n",
    "drt = DecisionTreeRegressor()\n",
    "drt_param = {'criterion':['mse','friedman_mse','mae'],\n",
    "             'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "             'max_features': ['auto', 'sqrt'],\n",
    "             'min_samples_leaf': [1, 2, 4],\n",
    "             'min_samples_split': [2, 5, 10]}\n",
    "\n",
    "ridge = Ridge()\n",
    "ridge_param = {'alpha':[0.1,0.2,0.5,0.8,1.0],\n",
    "               'solver':['auto','svd','lsqr']}\n",
    "\n",
    "bgr = BaggingRegressor()\n",
    "bgr_param = {'base_estimator': [LinearRegression(),Lasso(),RandomForestRegressor(),Ridge(),DecisionTreeRegressor()],\n",
    "             #'max_features':[1,2,5,7,10],\n",
    "             'n_estimators':[10,20,50,100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramtable = pd.DataFrame(columns = ['model', 'parameters', 'acc score'])\n",
    "def gridsearch(cv_model, parameters):\n",
    "    rsmodel = GridSearchCV (estimator = cv_model, \n",
    "                            param_grid = parameters, \n",
    "                            cv = 5, \n",
    "                            verbose=2, \n",
    "                            n_jobs = -1)\n",
    "    \n",
    "    rsmodel.fit(X_train, y_train)\n",
    "    \n",
    "    result = ({'model': cv_model,\n",
    "               'parameters': rsmodel.best_params_,\n",
    "               'acc score': rsmodel.best_score_})\n",
    "    \n",
    "    global paramtable\n",
    "    paramtable = paramtable.append(result, ignore_index=True)\n",
    "    return paramtable\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch(rfr,rfr_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
